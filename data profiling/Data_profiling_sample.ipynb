{"cells":[{"cell_type":"markdown","source":["### Data Profiling"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be08bd4f-7f2f-4759-8600-b9a72cb0d027","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"88b4b542-8e9c-4a3a-85de-6d12f724ceb9","inputWidgets":{},"title":"Import modules"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def create_df(schema,path):\n    return spark.read.format('csv').option('header',True).schema(schema).load(path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"cc5209df-74da-46a3-aa7f-4fd61998756d","inputWidgets":{},"title":"Read the Dataframe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndef get_null_perc(df, null_cols):\n    schema = StructType([ \\\n        StructField(\"Column\",StringType(),True), \\\n        StructField(\"NullPercentage\",StringType(),True)\n    ])\n    emptyRDD = spark.sparkContext.emptyRDD()\n    resultdf = spark.createDataFrame(emptyRDD, schema=schema)\n    for x in null_cols:\n        if x.upper() in (name.upper() for name in df.columns):\n            df_null_count = df.select(F.col(x)).filter(F.col(x).isNull() | (F.col(x) == '')).count()\n            df_null = spark.createDataFrame([[x, str(df_null_count*100.0/df.count()) + '%' ]],schema=schema)\n            resultdf = resultdf.union(df_null)\n    return resultdf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"246f608e-62d4-4934-9d77-999c0908b1ce","inputWidgets":{},"title":"Check the Null values Percentage in columns "}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_summary_numeric(df, numeric_cols):\n    return df.select(numeric_cols).summary()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"00ccffd2-38db-434f-8a7c-1db42ed4f457","inputWidgets":{},"title":"Get the Summary  of Numeric columns"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_distinct_counts(df, aggregate_cols):\n    schema = StructType([ \\\n        StructField(\"Column\",StringType(),True), \\\n        StructField(\"DistinctCount\",StringType(),True)\n    ])\n\n    emptyRDD = spark.sparkContext.emptyRDD()\n    resultdf = spark.createDataFrame(emptyRDD, schema=schema)\n\n    for x in aggregate_cols:\n        if x.upper() in (name.upper() for name in df.columns):\n            df_distinct_count = df.select(F.col(x)).distinct().count()\n            df_distinct = spark.createDataFrame([[x, str(df_distinct_count)]],schema=schema)\n            resultdf = resultdf.union(df_distinct)\n\n    return resultdf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"bddf6737-bcd6-44d9-8593-85cbcdb615b8","inputWidgets":{},"title":"Distinct Values in columns"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_distribution_counts(df, aggregate_cols):\n    result = []\n    for x in aggregate_cols:\n        if x.upper() in (name.upper() for name in df.columns):\n            result.append(df.groupby(F.col(x)).count().sort(F.col(\"count\").desc()))\n    return result\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"0a6f5ddd-b735-4182-900b-df19e3969047","inputWidgets":{},"title":"Distribution for Aggregate Columns"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_mismatch_perc(spark, df, data_quality_cols_regex):\n    schema = StructType([ \\\n        StructField(\"Column\",StringType(),True), \\\n        StructField(\"MismatchPercentage\",StringType(),True)\n    ])\n\n    emptyRDD = spark.sparkContext.emptyRDD()\n    resultdf = spark.createDataFrame(emptyRDD, schema=schema)\n\n\n    for key, value in data_quality_cols_regex.items():\n        if key.upper() in (name.upper() for name in df.columns):\n            df_regex_not_like_count = df.select(F.col(key)).filter(~F.col(key).rlike(value)).count()\n            df_regex_not_like = spark.createDataFrame([[key, str(df_regex_not_like_count*100.0/df.count()) + '%']],schema=schema)\n            resultdf = resultdf.union(df_regex_not_like)\n\n    return resultdf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"1914b03f-5147-4700-9b4d-b9596d85b491","inputWidgets":{},"title":"Data Quality Mismatch Percentage"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Data_profiling_sample","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":-1,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":4012352398297113}},"nbformat":4,"nbformat_minor":0}
